{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import MovieLens\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--use_weighted_loss', action='store_true',\n",
    "#                     help='Whether to use weighted MSE loss.')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda:3')\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '')\n",
    "dataset = MovieLens('/home/yanmy/GEIL/PyG/ml-latest-small', model_name='/home/yanmy/sentence_transformer_model/all-mpnet')\n",
    "data = dataset[0].to(device)\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add user node features for message passing:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_nodes, device\u001b[38;5;241m=\u001b[39m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_nodes\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add user node features for message passing:\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)\n",
    "\n",
    "# We have an unbalanced dataset with many labels for rating 3 and 4, and very\n",
    "# few for 0 and 1. Therefore we use a weighted MSE loss.\n",
    "if True:\n",
    "    weight = torch.bincount(train_data['user', 'movie'].edge_label)\n",
    "    weight = weight.max() / weight\n",
    "else:\n",
    "    weight = None\n",
    "\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()\n",
    "\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['user'][row], z_dict['movie'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "# class Model(torch.nn.Module):\n",
    "#     def __init__(self, hidden_channels):\n",
    "#         super().__init__()\n",
    "#         self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "#         self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "#         self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "#     def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "#         z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "#         return self.decoder(z_dict, edge_label_index)\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "    def get_embeddings(self, x_dict, edge_index_dict):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return z_dict\n",
    "\n",
    "\n",
    "model = Model(hidden_channels=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['user', 'movie'].edge_label_index)\n",
    "    target = train_data['user', 'movie'].edge_label\n",
    "    loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['user', 'movie'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data['user', 'movie'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)\n",
    "\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    test_rmse = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "          f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')\n",
    "node_embeddings = model.get_embeddings(data.x_dict, data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9742, 64]), torch.Size([610, 64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embeddings['movie'].shape,node_embeddings['user'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hospital_dirty = pd.read_csv('../GEIL_Data/hospital/original/dirty.csv').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>provider_number</th>\n",
       "      <th>name</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>address_3</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>county</th>\n",
       "      <th>phone</th>\n",
       "      <th>type</th>\n",
       "      <th>owner</th>\n",
       "      <th>emergency_service</th>\n",
       "      <th>condition</th>\n",
       "      <th>measure_code</th>\n",
       "      <th>measure_name</th>\n",
       "      <th>score</th>\n",
       "      <th>sample</th>\n",
       "      <th>state_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2053258100</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-card-2</td>\n",
       "      <td>surgery patients who were taking heart drugs c...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-card-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2053258100</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-1</td>\n",
       "      <td>surgery patients who were given an antibiotic ...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-inf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2053258100</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-2</td>\n",
       "      <td>surgery patients who were given the  right kin...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-inf-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birminghxm</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2053258100</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-3</td>\n",
       "      <td>surgery patients whose preventive antibiotics ...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-inf-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2053258100</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-4</td>\n",
       "      <td>all heart surgery patients whose blood sugar (...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-inf-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>2052743000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pn-6</td>\n",
       "      <td>pneumonia patients given the most appropriate ...</td>\n",
       "      <td>77%</td>\n",
       "      <td>74 patients</td>\n",
       "      <td>al_pn-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>2052743000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pn-7</td>\n",
       "      <td>pneumonia patients assessed and given influenz...</td>\n",
       "      <td>64%</td>\n",
       "      <td>55 patients</td>\n",
       "      <td>al_pn-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>2052743000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-card-2</td>\n",
       "      <td>surgery pxtients who were txking hexrt drugs c...</td>\n",
       "      <td>25%</td>\n",
       "      <td>8 patients</td>\n",
       "      <td>al_scip-card-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>2052743000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-1</td>\n",
       "      <td>surgery patients who were given an antibiotic ...</td>\n",
       "      <td>64%</td>\n",
       "      <td>28 patients</td>\n",
       "      <td>al_scip-inf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>2052743000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-2</td>\n",
       "      <td>surgery patients who were given the  right kin...</td>\n",
       "      <td>88%</td>\n",
       "      <td>25 patients</td>\n",
       "      <td>al_scip-inf-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index provider_number                              name  \\\n",
       "0       1           10018  callahan eye foundation hospital   \n",
       "1       2           10018  callahan eye foundation hospital   \n",
       "2       3           10018  callahan eye foundation hospital   \n",
       "3       4           10018  callahan eye foundation hospital   \n",
       "4       5           10018  callahan eye foundation hospital   \n",
       "..    ...             ...                               ...   \n",
       "995   996           10050                st vincents blount   \n",
       "996   997           10050                st vincents blount   \n",
       "997   998           10050                st vincents blount   \n",
       "998   999           10050                st vincents blount   \n",
       "999  1000           10050                st vincents blount   \n",
       "\n",
       "                address_1 address_2 address_3        city state    zip  \\\n",
       "0    1720 university blvd     empty     empty  birmingham    al  35233   \n",
       "1    1720 university blvd     empty     empty  birmingham    al  35233   \n",
       "2    1720 university blvd     empty     empty  birmingham    al  35233   \n",
       "3    1720 university blvd     empty     empty  birminghxm    al  35233   \n",
       "4    1720 university blvd     empty     empty  birmingham    al  35233   \n",
       "..                    ...       ...       ...         ...   ...    ...   \n",
       "995   150 gilbreath drive     empty     empty     oneonta    al  35121   \n",
       "996   150 gilbreath drive     empty     empty     oneonta    al  35121   \n",
       "997   150 gilbreath drive     empty     empty     oneonta    al  35121   \n",
       "998   150 gilbreath drive     empty     empty     oneonta    al  35121   \n",
       "999   150 gilbreath drive     empty     empty     oneonta    al  35121   \n",
       "\n",
       "        county       phone                  type  \\\n",
       "0    jefferson  2053258100  acute care hospitals   \n",
       "1    jefferson  2053258100  acute care hospitals   \n",
       "2    jefferson  2053258100  acute care hospitals   \n",
       "3    jefferson  2053258100  acute care hospitals   \n",
       "4    jefferson  2053258100  acute care hospitals   \n",
       "..         ...         ...                   ...   \n",
       "995     blount  2052743000  acute care hospitals   \n",
       "996     blount  2052743000  acute care hospitals   \n",
       "997     blount  2052743000  acute care hospitals   \n",
       "998     blount  2052743000  acute care hospitals   \n",
       "999     blount  2052743000  acute care hospitals   \n",
       "\n",
       "                              owner emergency_service  \\\n",
       "0    voluntary non-profit - private               yes   \n",
       "1    voluntary non-profit - private               yes   \n",
       "2    voluntary non-profit - private               yes   \n",
       "3    voluntary non-profit - private               yes   \n",
       "4    voluntary non-profit - private               yes   \n",
       "..                              ...               ...   \n",
       "995  voluntary non-profit - private               yes   \n",
       "996  voluntary non-profit - private               yes   \n",
       "997  voluntary non-profit - private               yes   \n",
       "998  voluntary non-profit - private               yes   \n",
       "999  voluntary non-profit - private               yes   \n",
       "\n",
       "                         condition measure_code  \\\n",
       "0    surgical infection prevention  scip-card-2   \n",
       "1    surgical infection prevention   scip-inf-1   \n",
       "2    surgical infection prevention   scip-inf-2   \n",
       "3    surgical infection prevention   scip-inf-3   \n",
       "4    surgical infection prevention   scip-inf-4   \n",
       "..                             ...          ...   \n",
       "995                      pneumonia         pn-6   \n",
       "996                      pneumonia         pn-7   \n",
       "997  surgical infection prevention  scip-card-2   \n",
       "998  surgical infection prevention   scip-inf-1   \n",
       "999  surgical infection prevention   scip-inf-2   \n",
       "\n",
       "                                          measure_name  score       sample  \\\n",
       "0    surgery patients who were taking heart drugs c...  empty        empty   \n",
       "1    surgery patients who were given an antibiotic ...  empty        empty   \n",
       "2    surgery patients who were given the  right kin...  empty        empty   \n",
       "3    surgery patients whose preventive antibiotics ...  empty        empty   \n",
       "4    all heart surgery patients whose blood sugar (...  empty        empty   \n",
       "..                                                 ...    ...          ...   \n",
       "995  pneumonia patients given the most appropriate ...    77%  74 patients   \n",
       "996  pneumonia patients assessed and given influenz...    64%  55 patients   \n",
       "997  surgery pxtients who were txking hexrt drugs c...    25%   8 patients   \n",
       "998  surgery patients who were given an antibiotic ...    64%  28 patients   \n",
       "999  surgery patients who were given the  right kin...    88%  25 patients   \n",
       "\n",
       "      state_average  \n",
       "0    al_scip-card-2  \n",
       "1     al_scip-inf-1  \n",
       "2     al_scip-inf-2  \n",
       "3     al_scip-inf-3  \n",
       "4     al_scip-inf-4  \n",
       "..              ...  \n",
       "995         al_pn-6  \n",
       "996         al_pn-7  \n",
       "997  al_scip-card-2  \n",
       "998   al_scip-inf-1  \n",
       "999   al_scip-inf-2  \n",
       "\n",
       "[1000 rows x 20 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_data = list(hospital_dirty.columns)\n",
    "all_data.extend(list(set(hospital_dirty.values.flatten())))\n",
    "# all_data\n",
    "unique_dict = {item: index for index, item in enumerate(all_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_list = []\n",
    "for index,row in hospital_dirty.iterrows():\n",
    "    for x,y in row[1:].items(): ## Skip Index\n",
    "        triple_list.append([index,float(unique_dict[x]),unique_dict[y]])\n",
    "triple_pd = pd.DataFrame(triple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df.columns = ['title']\n",
    "entity_df['movieId'] = entity_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df.to_csv('hospital/entity_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_pd.columns = ['userId','rating','movieId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_pd.to_csv('hospital/triple_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18995</th>\n",
       "      <td>999</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18996</th>\n",
       "      <td>999</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18997</th>\n",
       "      <td>999</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18998</th>\n",
       "      <td>999</td>\n",
       "      <td>18.0</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18999</th>\n",
       "      <td>999</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  rating  movieId\n",
       "0           0     1.0      450\n",
       "1           0     2.0     1818\n",
       "2           0     3.0      460\n",
       "3           0     4.0      285\n",
       "4           0     5.0      285\n",
       "...       ...     ...      ...\n",
       "18995     999    15.0     1715\n",
       "18996     999    16.0     1227\n",
       "18997     999    17.0     1745\n",
       "18998     999    18.0      280\n",
       "18999     999    19.0     1345\n",
       "\n",
       "[19000 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07835f99ecff486cb408ac6def1761d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  user={ num_nodes=610 },\n",
      "  movie={ x=[9742, 768] },\n",
      "  (user, rates, movie)={\n",
      "    edge_index=[2, 100836],\n",
      "    edge_label=[100836, 1],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch_geometric.data import HeteroData, download_url, extract_zip\n",
    "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
    "\n",
    "movie_path = osp.join( 'ml-latest-small', 'movies.csv')\n",
    "rating_path = osp.join( 'ml-latest-small', 'ratings.csv')\n",
    "# movie_path = osp.join( 'hospital', 'entity_df.csv')\n",
    "# rating_path = osp.join( 'hospital', 'triple_df.csv')\n",
    "\n",
    "\n",
    "def load_node_csv(path, index_col, encoders=None, **kwargs):\n",
    "    df = pd.read_csv(path, index_col=index_col, **kwargs)\n",
    "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
    "\n",
    "    x = None\n",
    "    if encoders is not None:\n",
    "        xs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        x = torch.cat(xs, dim=-1)\n",
    "\n",
    "    return x, mapping\n",
    "\n",
    "\n",
    "def load_edge_csv(path, src_index_col, src_mapping, dst_index_col, dst_mapping,\n",
    "                  encoders=None, **kwargs):\n",
    "    df = pd.read_csv(path, **kwargs)\n",
    "    # df['rating'] = df['rating'].astype(float)\n",
    "    src = [src_mapping[index] for index in df[src_index_col]]\n",
    "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
    "    edge_index = torch.tensor([src, dst])\n",
    "\n",
    "    edge_attr = None\n",
    "    if encoders is not None:\n",
    "        edge_attrs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        edge_attr = torch.cat(edge_attrs, dim=-1)\n",
    "\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "\n",
    "class SequenceEncoder:\n",
    "    # The 'SequenceEncoder' encodes raw column strings into embeddings.\n",
    "    def __init__(self, model_name='/home/yanmy/sentence_transformer_model/all-mpnet/', device=None):\n",
    "        self.device = device\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, df):\n",
    "        x = self.model.encode(df.values, show_progress_bar=True,\n",
    "                              convert_to_tensor=True, device=self.device)\n",
    "        return x.cpu()\n",
    "\n",
    "\n",
    "class GenresEncoder:\n",
    "    # The 'GenreEncoder' splits the raw column strings by 'sep' and converts\n",
    "    # individual elements to categorical labels.\n",
    "    def __init__(self, sep='|'):\n",
    "        self.sep = sep\n",
    "\n",
    "    def __call__(self, df):\n",
    "        genres = {g for col in df.values for g in col.split(self.sep)}\n",
    "        mapping = {genre: i for i, genre in enumerate(genres)}\n",
    "\n",
    "        x = torch.zeros(len(df), len(mapping))\n",
    "        for i, col in enumerate(df.values):\n",
    "            for genre in col.split(self.sep):\n",
    "                x[i, mapping[genre]] = 1\n",
    "        return x\n",
    "\n",
    "\n",
    "class IdentityEncoder:\n",
    "    # The 'IdentityEncoder' takes the raw column values and converts them to\n",
    "    # PyTorch tensors.\n",
    "    def __init__(self, dtype=None):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, df):\n",
    "        return torch.from_numpy(df.values).view(-1, 1).to(self.dtype)\n",
    "\n",
    "\n",
    "user_x, user_mapping = load_node_csv(rating_path, index_col='userId')\n",
    "\n",
    "movie_x, movie_mapping = load_node_csv(\n",
    "    movie_path, index_col='movieId', encoders={\n",
    "        'title': SequenceEncoder()\n",
    "    })\n",
    "\n",
    "edge_index, edge_label = load_edge_csv(\n",
    "    rating_path,\n",
    "    src_index_col='userId',\n",
    "    src_mapping=user_mapping,\n",
    "    dst_index_col='movieId',\n",
    "    dst_mapping=movie_mapping,\n",
    "    encoders={'rating': IdentityEncoder(dtype=torch.long)},\n",
    "    # encoders={'rating': SequenceEncoder()},\n",
    ")\n",
    "\n",
    "data = HeteroData()\n",
    "data['user'].num_nodes = len(user_mapping)  # Users do not have any features.\n",
    "data['movie'].x = movie_x\n",
    "data['user', 'rates', 'movie'].edge_index = edge_index\n",
    "data['user', 'rates', 'movie'].edge_label = edge_label\n",
    "print(data)\n",
    "\n",
    "# We can now convert `data` into an appropriate format for training a\n",
    "# graph-based machine learning model:\n",
    "\n",
    "# 1. Add a reverse ('movie', 'rev_rates', 'user') relation for message passing.\n",
    "data = ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "# 2. Perform a link-level split into training, validation, and test edges.\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.05,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")\n",
    "train_data, val_data, test_data = transform(data)\n",
    "# print(train_data)\n",
    "# print(val_data)\n",
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import MovieLens\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--use_weighted_loss', action='store_true',\n",
    "#                     help='Whether to use weighted MSE loss.')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda:3')\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '')\n",
    "dataset = MovieLens('/home/yanmy/GEIL/PyG/ml-latest-small', model_name='/home/yanmy/sentence_transformer_model/all-mpnet')\n",
    "data = dataset[0].to(device)\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import (\n",
    "    HeteroData,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    "    extract_zip,\n",
    ")\n",
    "\n",
    "\n",
    "class HospitalData(InMemoryDataset):\n",
    "    r\"\"\"A heterogeneous rating dataset, assembled by GroupLens Research from\n",
    "    the `MovieLens web site <https://movielens.org>`_, consisting of nodes of\n",
    "    type :obj:`\"movie\"` and :obj:`\"user\"`.\n",
    "    User ratings for movies are available as ground truth labels for the edges\n",
    "    between the users and the movies :obj:`(\"user\", \"rates\", \"movie\")`.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            every access. (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "        model_name (str): Name of model used to transform movie titles to node\n",
    "            features. The model comes from the`Huggingface SentenceTransformer\n",
    "            <https://huggingface.co/sentence-transformers>`_.\n",
    "        force_reload (bool, optional): Whether to re-process the dataset.\n",
    "            (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "        model_name: Optional[str] = '/home/yanmy/sentence_transformer_model/all-mpnet/',\n",
    "        force_reload: bool = False,\n",
    "    ) -> None:\n",
    "        self.model_name = model_name\n",
    "        super().__init__(root, transform, pre_transform,\n",
    "                         force_reload=force_reload)\n",
    "        self.load(self.processed_paths[0], data_cls=HeteroData)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return [\n",
    "            osp.join('hospital', 'entity_df.csv'),\n",
    "            osp.join('hospital', 'triple_df.csv'),\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return f'data_{self.model_name}.pt'\n",
    "\n",
    "    def download(self) -> None:\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.remove(path)\n",
    "\n",
    "    def process(self) -> None:\n",
    "        import pandas as pd\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        data = HeteroData()\n",
    "\n",
    "        df = pd.read_csv(self.raw_paths[0], index_col='movieId')\n",
    "        movie_mapping = {idx: i for i, idx in enumerate(df.index)}\n",
    "\n",
    "        # genres = df['genres'].str.get_dummies('|').values\n",
    "        # genres = torch.from_numpy(genres).to(torch.float)\n",
    "\n",
    "        model = SentenceTransformer(self.model_name)\n",
    "        with torch.no_grad():\n",
    "            emb = model.encode(df['title'].values, show_progress_bar=True,\n",
    "                               convert_to_tensor=True).cpu()\n",
    "\n",
    "        data['movie'].x = torch.cat([emb], dim=-1)\n",
    "\n",
    "        df = pd.read_csv(self.raw_paths[1])\n",
    "        user_mapping = {idx: i for i, idx in enumerate(df['userId'].unique())}\n",
    "        data['user'].num_nodes = len(user_mapping)\n",
    "\n",
    "        src = [user_mapping[idx] for idx in df['userId']]\n",
    "        dst = [movie_mapping[idx] for idx in df['movieId']]\n",
    "        edge_index = torch.tensor([src, dst])\n",
    "\n",
    "        rating = torch.from_numpy(df['rating'].values).to(torch.long)\n",
    "        # time = torch.from_numpy(df['timestamp'].values).to(torch.long)\n",
    "\n",
    "        data['user', 'rates', 'movie'].edge_index = edge_index\n",
    "        data['user', 'rates', 'movie'].edge_label = rating\n",
    "        # data['user', 'rates', 'movie'].time = time\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        self.save([data], self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  movie={ x=[2185, 1024] },\n",
       "  user={ x=[1000, 1000] },\n",
       "  (user, rates, movie)={\n",
       "    edge_index=[2, 19000],\n",
       "    edge_label=[19000],\n",
       "  },\n",
       "  (movie, rev_rates, user)={ edge_index=[2, 19000] }\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc9784f30b04310bf112e02b4c364ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = HospitalData('/home/yanmy/GEIL/PyG/hospital', model_name='/home/yanmy/sentence_transformer_model/bge-large-en-1.5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0].to(device)\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label.\n",
    "\n",
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 132.7034, Train: 10.2948, Val: 10.2341, Test: 10.4015\n",
      "Epoch: 002, Loss: 109.5014, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 003, Loss: 31.9396, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 004, Loss: 407.3914, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 005, Loss: 36.1965, Train: 8.0934, Val: 8.0816, Test: 8.2086\n",
      "Epoch: 006, Loss: 67.6914, Train: 10.1806, Val: 10.1237, Test: 10.2908\n",
      "Epoch: 007, Loss: 107.0868, Train: 10.6294, Val: 10.5652, Test: 10.7380\n",
      "Epoch: 008, Loss: 116.7331, Train: 10.6394, Val: 10.5757, Test: 10.7490\n",
      "Epoch: 009, Loss: 116.9525, Train: 10.5877, Val: 10.5258, Test: 10.6990\n",
      "Epoch: 010, Loss: 115.8193, Train: 10.5542, Val: 10.4936, Test: 10.6668\n",
      "Epoch: 011, Loss: 115.0907, Train: 10.4934, Val: 10.4339, Test: 10.6069\n",
      "Epoch: 012, Loss: 113.7693, Train: 10.3396, Val: 10.2832, Test: 10.4551\n",
      "Epoch: 013, Loss: 110.4612, Train: 10.0114, Val: 9.9620, Test: 10.1308\n",
      "Epoch: 014, Loss: 103.5649, Train: 9.3636, Val: 9.3289, Test: 9.4901\n",
      "Epoch: 015, Loss: 90.6041, Train: 8.1692, Val: 8.1654, Test: 8.3078\n",
      "Epoch: 016, Loss: 68.9780, Train: 7.4126, Val: 7.3871, Test: 7.4759\n",
      "Epoch: 017, Loss: 40.9408, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 018, Loss: 24.6832, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 019, Loss: 52.2150, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 020, Loss: 60.0663, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 021, Loss: 32.1582, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 022, Loss: 21.8627, Train: 7.4126, Val: 7.3861, Test: 7.4750\n",
      "Epoch: 023, Loss: 28.8755, Train: 7.4130, Val: 7.3878, Test: 7.4782\n",
      "Epoch: 024, Loss: 36.9333, Train: 7.4138, Val: 7.3887, Test: 7.4839\n",
      "Epoch: 025, Loss: 39.2959, Train: 7.4128, Val: 7.3871, Test: 7.4805\n",
      "Epoch: 026, Loss: 35.2635, Train: 7.4124, Val: 7.3862, Test: 7.4752\n",
      "Epoch: 027, Loss: 26.6212, Train: 7.4126, Val: 7.3861, Test: 7.4749\n",
      "Epoch: 028, Loss: 17.6192, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 029, Loss: 14.9831, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 030, Loss: 21.1291, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 031, Loss: 24.5619, Train: 7.4126, Val: 7.3862, Test: 7.4750\n",
      "Epoch: 032, Loss: 17.8101, Train: 7.4126, Val: 7.3861, Test: 7.4749\n",
      "Epoch: 033, Loss: 11.2283, Train: 7.4124, Val: 7.3861, Test: 7.4748\n",
      "Epoch: 034, Loss: 11.1986, Train: 7.4102, Val: 7.3833, Test: 7.4765\n",
      "Epoch: 035, Loss: 13.9582, Train: 7.4113, Val: 7.3832, Test: 7.4818\n",
      "Epoch: 036, Loss: 15.0524, Train: 7.4114, Val: 7.3831, Test: 7.4827\n",
      "Epoch: 037, Loss: 13.2720, Train: 7.4079, Val: 7.3803, Test: 7.4763\n",
      "Epoch: 038, Loss: 9.9369, Train: 7.4080, Val: 7.3821, Test: 7.4730\n",
      "Epoch: 039, Loss: 7.7907, Train: 7.4106, Val: 7.3851, Test: 7.4740\n",
      "Epoch: 040, Loss: 8.7879, Train: 7.4116, Val: 7.3858, Test: 7.4746\n",
      "Epoch: 041, Loss: 11.0050, Train: 7.4102, Val: 7.3848, Test: 7.4740\n",
      "Epoch: 042, Loss: 10.5903, Train: 7.4030, Val: 7.3785, Test: 7.4679\n",
      "Epoch: 043, Loss: 8.1640, Train: 7.3988, Val: 7.3720, Test: 7.4676\n",
      "Epoch: 044, Loss: 6.9886, Train: 7.4043, Val: 7.3763, Test: 7.4773\n",
      "Epoch: 045, Loss: 7.6435, Train: 7.4102, Val: 7.3824, Test: 7.4857\n",
      "Epoch: 046, Loss: 8.5588, Train: 7.4075, Val: 7.3797, Test: 7.4827\n",
      "Epoch: 047, Loss: 8.4585, Train: 7.3963, Val: 7.3693, Test: 7.4683\n",
      "Epoch: 048, Loss: 7.3141, Train: 7.3848, Val: 7.3587, Test: 7.4521\n",
      "Epoch: 049, Loss: 6.1043, Train: 7.3829, Val: 7.3587, Test: 7.4494\n",
      "Epoch: 050, Loss: 5.8693, Train: 7.3876, Val: 7.3648, Test: 7.4542\n",
      "Epoch: 051, Loss: 6.4949, Train: 7.3890, Val: 7.3664, Test: 7.4556\n",
      "Epoch: 052, Loss: 6.6950, Train: 7.3849, Val: 7.3623, Test: 7.4515\n",
      "Epoch: 053, Loss: 5.9343, Train: 7.3777, Val: 7.3543, Test: 7.4448\n",
      "Epoch: 054, Loss: 5.1607, Train: 7.3716, Val: 7.3473, Test: 7.4387\n",
      "Epoch: 055, Loss: 5.1243, Train: 7.3681, Val: 7.3434, Test: 7.4351\n",
      "Epoch: 056, Loss: 5.4894, Train: 7.3662, Val: 7.3415, Test: 7.4334\n",
      "Epoch: 057, Loss: 5.5871, Train: 7.3661, Val: 7.3417, Test: 7.4333\n",
      "Epoch: 058, Loss: 5.2183, Train: 7.3685, Val: 7.3451, Test: 7.4360\n",
      "Epoch: 059, Loss: 4.7488, Train: 7.3731, Val: 7.3504, Test: 7.4401\n",
      "Epoch: 060, Loss: 4.6389, Train: 7.3769, Val: 7.3546, Test: 7.4438\n",
      "Epoch: 061, Loss: 4.8589, Train: 7.3771, Val: 7.3548, Test: 7.4440\n",
      "Epoch: 062, Loss: 4.9213, Train: 7.3729, Val: 7.3504, Test: 7.4399\n",
      "Epoch: 063, Loss: 4.6230, Train: 7.3664, Val: 7.3436, Test: 7.4338\n",
      "Epoch: 064, Loss: 4.3094, Train: 7.3604, Val: 7.3368, Test: 7.4280\n",
      "Epoch: 065, Loss: 4.2636, Train: 7.3565, Val: 7.3321, Test: 7.4242\n",
      "Epoch: 066, Loss: 4.3540, Train: 7.3544, Val: 7.3297, Test: 7.4222\n",
      "Epoch: 067, Loss: 4.3131, Train: 7.3536, Val: 7.3289, Test: 7.4215\n",
      "Epoch: 068, Loss: 4.0949, Train: 7.3539, Val: 7.3295, Test: 7.4217\n",
      "Epoch: 069, Loss: 3.8898, Train: 7.3546, Val: 7.3305, Test: 7.4222\n",
      "Epoch: 070, Loss: 3.8558, Train: 7.3543, Val: 7.3303, Test: 7.4218\n",
      "Epoch: 071, Loss: 3.9033, Train: 7.3522, Val: 7.3279, Test: 7.4198\n",
      "Epoch: 072, Loss: 3.8422, Train: 7.3489, Val: 7.3239, Test: 7.4167\n",
      "Epoch: 073, Loss: 3.6787, Train: 7.3452, Val: 7.3198, Test: 7.4136\n",
      "Epoch: 074, Loss: 3.5740, Train: 7.3423, Val: 7.3165, Test: 7.4110\n",
      "Epoch: 075, Loss: 3.5772, Train: 7.3404, Val: 7.3143, Test: 7.4094\n",
      "Epoch: 076, Loss: 3.5843, Train: 7.3393, Val: 7.3131, Test: 7.4083\n",
      "Epoch: 077, Loss: 3.5122, Train: 7.3387, Val: 7.3126, Test: 7.4076\n",
      "Epoch: 078, Loss: 3.4038, Train: 7.3387, Val: 7.3126, Test: 7.4073\n",
      "Epoch: 079, Loss: 3.3488, Train: 7.3385, Val: 7.3126, Test: 7.4072\n",
      "Epoch: 080, Loss: 3.3504, Train: 7.3378, Val: 7.3118, Test: 7.4065\n",
      "Epoch: 081, Loss: 3.3285, Train: 7.3364, Val: 7.3102, Test: 7.4052\n",
      "Epoch: 082, Loss: 3.2530, Train: 7.3347, Val: 7.3083, Test: 7.4037\n",
      "Epoch: 083, Loss: 3.1827, Train: 7.3332, Val: 7.3066, Test: 7.4024\n",
      "Epoch: 084, Loss: 3.1598, Train: 7.3321, Val: 7.3054, Test: 7.4015\n",
      "Epoch: 085, Loss: 3.1508, Train: 7.3315, Val: 7.3047, Test: 7.4010\n",
      "Epoch: 086, Loss: 3.1115, Train: 7.3313, Val: 7.3046, Test: 7.4008\n",
      "Epoch: 087, Loss: 3.0530, Train: 7.3313, Val: 7.3047, Test: 7.4007\n",
      "Epoch: 088, Loss: 3.0160, Train: 7.3313, Val: 7.3047, Test: 7.4006\n",
      "Epoch: 089, Loss: 3.0048, Train: 7.3308, Val: 7.3042, Test: 7.4002\n",
      "Epoch: 090, Loss: 2.9846, Train: 7.3299, Val: 7.3033, Test: 7.3994\n",
      "Epoch: 091, Loss: 2.9425, Train: 7.3289, Val: 7.3020, Test: 7.3985\n",
      "Epoch: 092, Loss: 2.9041, Train: 7.3279, Val: 7.3009, Test: 7.3977\n",
      "Epoch: 093, Loss: 2.8858, Train: 7.3271, Val: 7.3001, Test: 7.3971\n",
      "Epoch: 094, Loss: 2.8708, Train: 7.3267, Val: 7.2997, Test: 7.3967\n",
      "Epoch: 095, Loss: 2.8419, Train: 7.3266, Val: 7.2996, Test: 7.3966\n",
      "Epoch: 096, Loss: 2.8081, Train: 7.3265, Val: 7.2995, Test: 7.3966\n",
      "Epoch: 097, Loss: 2.7863, Train: 7.3263, Val: 7.2993, Test: 7.3964\n",
      "Epoch: 098, Loss: 2.7727, Train: 7.3259, Val: 7.2988, Test: 7.3960\n",
      "Epoch: 099, Loss: 2.7523, Train: 7.3252, Val: 7.2981, Test: 7.3954\n",
      "Epoch: 100, Loss: 2.7252, Train: 7.3246, Val: 7.2973, Test: 7.3948\n",
      "Epoch: 101, Loss: 2.7041, Train: 7.3240, Val: 7.2967, Test: 7.3943\n",
      "Epoch: 102, Loss: 2.6907, Train: 7.3236, Val: 7.2963, Test: 7.3940\n",
      "Epoch: 103, Loss: 2.6749, Train: 7.3234, Val: 7.2961, Test: 7.3938\n",
      "Epoch: 104, Loss: 2.6534, Train: 7.3233, Val: 7.2959, Test: 7.3937\n",
      "Epoch: 105, Loss: 2.6338, Train: 7.3231, Val: 7.2958, Test: 7.3936\n",
      "Epoch: 106, Loss: 2.6200, Train: 7.3229, Val: 7.2956, Test: 7.3934\n",
      "Epoch: 107, Loss: 2.6061, Train: 7.3225, Val: 7.2952, Test: 7.3931\n",
      "Epoch: 108, Loss: 2.5882, Train: 7.3221, Val: 7.2947, Test: 7.3928\n",
      "Epoch: 109, Loss: 2.5704, Train: 7.3218, Val: 7.2943, Test: 7.3925\n",
      "Epoch: 110, Loss: 2.5569, Train: 7.3215, Val: 7.2940, Test: 7.3922\n",
      "Epoch: 111, Loss: 2.5443, Train: 7.3213, Val: 7.2938, Test: 7.3921\n",
      "Epoch: 112, Loss: 2.5292, Train: 7.3212, Val: 7.2937, Test: 7.3920\n",
      "Epoch: 113, Loss: 2.5137, Train: 7.3211, Val: 7.2936, Test: 7.3919\n",
      "Epoch: 114, Loss: 2.5010, Train: 7.3209, Val: 7.2934, Test: 7.3917\n",
      "Epoch: 115, Loss: 2.4894, Train: 7.3207, Val: 7.2932, Test: 7.3916\n",
      "Epoch: 116, Loss: 2.4762, Train: 7.3205, Val: 7.2929, Test: 7.3913\n",
      "Epoch: 117, Loss: 2.4624, Train: 7.3202, Val: 7.2926, Test: 7.3911\n",
      "Epoch: 118, Loss: 2.4503, Train: 7.3200, Val: 7.2923, Test: 7.3909\n",
      "Epoch: 119, Loss: 2.4392, Train: 7.3198, Val: 7.2921, Test: 7.3908\n",
      "Epoch: 120, Loss: 2.4270, Train: 7.3197, Val: 7.2920, Test: 7.3907\n",
      "Epoch: 121, Loss: 2.4145, Train: 7.3196, Val: 7.2919, Test: 7.3906\n",
      "Epoch: 122, Loss: 2.4033, Train: 7.3194, Val: 7.2917, Test: 7.3904\n",
      "Epoch: 123, Loss: 2.3928, Train: 7.3193, Val: 7.2915, Test: 7.3903\n",
      "Epoch: 124, Loss: 2.3818, Train: 7.3191, Val: 7.2913, Test: 7.3901\n",
      "Epoch: 125, Loss: 2.3705, Train: 7.3189, Val: 7.2910, Test: 7.3900\n",
      "Epoch: 126, Loss: 2.3601, Train: 7.3187, Val: 7.2908, Test: 7.3898\n",
      "Epoch: 127, Loss: 2.3499, Train: 7.3185, Val: 7.2906, Test: 7.3897\n",
      "Epoch: 128, Loss: 2.3388, Train: 7.3184, Val: 7.2904, Test: 7.3896\n",
      "Epoch: 129, Loss: 2.3262, Train: 7.3182, Val: 7.2903, Test: 7.3896\n",
      "Epoch: 130, Loss: 2.3170, Train: 7.3181, Val: 7.2902, Test: 7.3895\n",
      "Epoch: 131, Loss: 2.3070, Train: 7.3180, Val: 7.2901, Test: 7.3894\n",
      "Epoch: 132, Loss: 2.2963, Train: 7.3179, Val: 7.2900, Test: 7.3893\n",
      "Epoch: 133, Loss: 2.2866, Train: 7.3178, Val: 7.2899, Test: 7.3892\n",
      "Epoch: 134, Loss: 2.2763, Train: 7.3177, Val: 7.2897, Test: 7.3892\n",
      "Epoch: 135, Loss: 2.2673, Train: 7.3176, Val: 7.2896, Test: 7.3892\n",
      "Epoch: 136, Loss: 2.2581, Train: 7.3175, Val: 7.2895, Test: 7.3891\n",
      "Epoch: 137, Loss: 2.2467, Train: 7.3174, Val: 7.2894, Test: 7.3890\n",
      "Epoch: 138, Loss: 2.2373, Train: 7.3173, Val: 7.2894, Test: 7.3889\n",
      "Epoch: 139, Loss: 2.2268, Train: 7.3171, Val: 7.2893, Test: 7.3889\n",
      "Epoch: 140, Loss: 2.2172, Train: 7.3170, Val: 7.2891, Test: 7.3887\n",
      "Epoch: 141, Loss: 2.2060, Train: 7.3168, Val: 7.2890, Test: 7.3885\n",
      "Epoch: 142, Loss: 2.1958, Train: 7.3167, Val: 7.2888, Test: 7.3884\n",
      "Epoch: 143, Loss: 2.1846, Train: 7.3165, Val: 7.2887, Test: 7.3883\n",
      "Epoch: 144, Loss: 2.1741, Train: 7.3165, Val: 7.2887, Test: 7.3881\n",
      "Epoch: 145, Loss: 2.1656, Train: 7.3163, Val: 7.2885, Test: 7.3881\n",
      "Epoch: 146, Loss: 2.1558, Train: 7.3162, Val: 7.2885, Test: 7.3880\n",
      "Epoch: 147, Loss: 2.1443, Train: 7.3161, Val: 7.2884, Test: 7.3879\n",
      "Epoch: 148, Loss: 2.1304, Train: 7.3160, Val: 7.2884, Test: 7.3879\n",
      "Epoch: 149, Loss: 2.1239, Train: 7.3158, Val: 7.2883, Test: 7.3878\n",
      "Epoch: 150, Loss: 2.1099, Train: 7.3157, Val: 7.2882, Test: 7.3877\n",
      "Epoch: 151, Loss: 2.0983, Train: 7.3157, Val: 7.2883, Test: 7.3878\n",
      "Epoch: 152, Loss: 2.0906, Train: 7.3155, Val: 7.2881, Test: 7.3876\n",
      "Epoch: 153, Loss: 2.0748, Train: 7.3154, Val: 7.2881, Test: 7.3875\n",
      "Epoch: 154, Loss: 2.0658, Train: 7.3154, Val: 7.2881, Test: 7.3876\n",
      "Epoch: 155, Loss: 2.0536, Train: 7.3152, Val: 7.2880, Test: 7.3874\n",
      "Epoch: 156, Loss: 2.0404, Train: 7.3151, Val: 7.2880, Test: 7.3873\n",
      "Epoch: 157, Loss: 2.0321, Train: 7.3150, Val: 7.2880, Test: 7.3873\n",
      "Epoch: 158, Loss: 2.0189, Train: 7.3149, Val: 7.2880, Test: 7.3872\n",
      "Epoch: 159, Loss: 2.0053, Train: 7.3148, Val: 7.2880, Test: 7.3871\n",
      "Epoch: 160, Loss: 1.9933, Train: 7.3148, Val: 7.2880, Test: 7.3872\n",
      "Epoch: 161, Loss: 1.9833, Train: 7.3148, Val: 7.2881, Test: 7.3872\n",
      "Epoch: 162, Loss: 1.9751, Train: 7.3147, Val: 7.2881, Test: 7.3872\n",
      "Epoch: 163, Loss: 1.9593, Train: 7.3147, Val: 7.2881, Test: 7.3872\n",
      "Epoch: 164, Loss: 1.9448, Train: 7.3146, Val: 7.2882, Test: 7.3872\n",
      "Epoch: 165, Loss: 1.9332, Train: 7.3147, Val: 7.2884, Test: 7.3874\n",
      "Epoch: 166, Loss: 1.9227, Train: 7.3146, Val: 7.2883, Test: 7.3872\n",
      "Epoch: 167, Loss: 1.9111, Train: 7.3146, Val: 7.2885, Test: 7.3873\n",
      "Epoch: 168, Loss: 1.8959, Train: 7.3145, Val: 7.2885, Test: 7.3873\n",
      "Epoch: 169, Loss: 1.8813, Train: 7.3145, Val: 7.2885, Test: 7.3873\n",
      "Epoch: 170, Loss: 1.8681, Train: 7.3144, Val: 7.2886, Test: 7.3873\n",
      "Epoch: 171, Loss: 1.8560, Train: 7.3145, Val: 7.2887, Test: 7.3872\n",
      "Epoch: 172, Loss: 1.8464, Train: 7.3144, Val: 7.2889, Test: 7.3875\n",
      "Epoch: 173, Loss: 1.8387, Train: 7.3146, Val: 7.2889, Test: 7.3874\n",
      "Epoch: 174, Loss: 1.8412, Train: 7.3143, Val: 7.2890, Test: 7.3876\n",
      "Epoch: 175, Loss: 1.8090, Train: 7.3142, Val: 7.2887, Test: 7.3871\n",
      "Epoch: 176, Loss: 1.7806, Train: 7.3142, Val: 7.2886, Test: 7.3871\n",
      "Epoch: 177, Loss: 1.7716, Train: 7.3141, Val: 7.2891, Test: 7.3877\n",
      "Epoch: 178, Loss: 1.7631, Train: 7.3141, Val: 7.2886, Test: 7.3869\n",
      "Epoch: 179, Loss: 1.7428, Train: 7.3138, Val: 7.2885, Test: 7.3869\n",
      "Epoch: 180, Loss: 1.7127, Train: 7.3136, Val: 7.2887, Test: 7.3872\n",
      "Epoch: 181, Loss: 1.7025, Train: 7.3140, Val: 7.2885, Test: 7.3867\n",
      "Epoch: 182, Loss: 1.6979, Train: 7.3132, Val: 7.2886, Test: 7.3871\n",
      "Epoch: 183, Loss: 1.6641, Train: 7.3132, Val: 7.2881, Test: 7.3865\n",
      "Epoch: 184, Loss: 1.6355, Train: 7.3133, Val: 7.2880, Test: 7.3863\n",
      "Epoch: 185, Loss: 1.6205, Train: 7.3125, Val: 7.2886, Test: 7.3872\n",
      "Epoch: 186, Loss: 1.6083, Train: 7.3135, Val: 7.2879, Test: 7.3861\n",
      "Epoch: 187, Loss: 1.6010, Train: 7.3121, Val: 7.2884, Test: 7.3872\n",
      "Epoch: 188, Loss: 1.5643, Train: 7.3124, Val: 7.2872, Test: 7.3858\n",
      "Epoch: 189, Loss: 1.5323, Train: 7.3121, Val: 7.2871, Test: 7.3859\n",
      "Epoch: 190, Loss: 1.5081, Train: 7.3115, Val: 7.2883, Test: 7.3870\n",
      "Epoch: 191, Loss: 1.4956, Train: 7.3125, Val: 7.2869, Test: 7.3853\n",
      "Epoch: 192, Loss: 1.4951, Train: 7.3112, Val: 7.2904, Test: 7.3881\n",
      "Epoch: 193, Loss: 1.4775, Train: 7.3127, Val: 7.2868, Test: 7.3850\n",
      "Epoch: 194, Loss: 1.4788, Train: 7.3105, Val: 7.2890, Test: 7.3870\n",
      "Epoch: 195, Loss: 1.4181, Train: 7.3103, Val: 7.2880, Test: 7.3862\n",
      "Epoch: 196, Loss: 1.3926, Train: 7.3114, Val: 7.2859, Test: 7.3848\n",
      "Epoch: 197, Loss: 1.3942, Train: 7.3104, Val: 7.2921, Test: 7.3900\n",
      "Epoch: 198, Loss: 1.3883, Train: 7.3119, Val: 7.2858, Test: 7.3842\n",
      "Epoch: 199, Loss: 1.4055, Train: 7.3098, Val: 7.2903, Test: 7.3884\n",
      "Epoch: 200, Loss: 1.3401, Train: 7.3098, Val: 7.2913, Test: 7.3896\n",
      "Epoch: 201, Loss: 1.3331, Train: 7.3115, Val: 7.2851, Test: 7.3838\n",
      "Epoch: 202, Loss: 1.3653, Train: 7.3098, Val: 7.2924, Test: 7.3914\n",
      "Epoch: 203, Loss: 1.3146, Train: 7.3094, Val: 7.2902, Test: 7.3888\n",
      "Epoch: 204, Loss: 1.2951, Train: 7.3100, Val: 7.2871, Test: 7.3854\n",
      "Epoch: 205, Loss: 1.2987, Train: 7.3098, Val: 7.2938, Test: 7.3935\n",
      "Epoch: 206, Loss: 1.2952, Train: 7.3105, Val: 7.2851, Test: 7.3841\n",
      "Epoch: 207, Loss: 1.3021, Train: 7.3095, Val: 7.2931, Test: 7.3927\n",
      "Epoch: 208, Loss: 1.2764, Train: 7.3093, Val: 7.2884, Test: 7.3870\n",
      "Epoch: 209, Loss: 1.2625, Train: 7.3090, Val: 7.2901, Test: 7.3891\n",
      "Epoch: 210, Loss: 1.2528, Train: 7.3090, Val: 7.2915, Test: 7.3907\n",
      "Epoch: 211, Loss: 1.2489, Train: 7.3095, Val: 7.2872, Test: 7.3856\n",
      "Epoch: 212, Loss: 1.2529, Train: 7.3097, Val: 7.2952, Test: 7.3953\n",
      "Epoch: 213, Loss: 1.2634, Train: 7.3111, Val: 7.2841, Test: 7.3828\n",
      "Epoch: 214, Loss: 1.3426, Train: 7.3123, Val: 7.3019, Test: 7.4030\n",
      "Epoch: 215, Loss: 1.3359, Train: 7.3113, Val: 7.2842, Test: 7.3825\n",
      "Epoch: 216, Loss: 1.4196, Train: 7.3088, Val: 7.2903, Test: 7.3892\n",
      "Epoch: 217, Loss: 1.2302, Train: 7.3188, Val: 7.3147, Test: 7.4168\n",
      "Epoch: 218, Loss: 1.4388, Train: 7.3120, Val: 7.2846, Test: 7.3829\n",
      "Epoch: 219, Loss: 1.5652, Train: 7.3110, Val: 7.2849, Test: 7.3832\n",
      "Epoch: 220, Loss: 1.2875, Train: 7.3262, Val: 7.3277, Test: 7.4295\n",
      "Epoch: 221, Loss: 1.5937, Train: 7.3105, Val: 7.2851, Test: 7.3831\n",
      "Epoch: 222, Loss: 1.2708, Train: 7.3113, Val: 7.2840, Test: 7.3821\n",
      "Epoch: 223, Loss: 1.4760, Train: 7.3100, Val: 7.2962, Test: 7.3960\n",
      "Epoch: 224, Loss: 1.2529, Train: 7.3195, Val: 7.3166, Test: 7.4183\n",
      "Epoch: 225, Loss: 1.4326, Train: 7.3103, Val: 7.2848, Test: 7.3828\n",
      "Epoch: 226, Loss: 1.2674, Train: 7.3108, Val: 7.2837, Test: 7.3818\n",
      "Epoch: 227, Loss: 1.3876, Train: 7.3094, Val: 7.2953, Test: 7.3946\n",
      "Epoch: 228, Loss: 1.2389, Train: 7.3157, Val: 7.3099, Test: 7.4110\n",
      "Epoch: 229, Loss: 1.3674, Train: 7.3093, Val: 7.2853, Test: 7.3829\n",
      "Epoch: 230, Loss: 1.2291, Train: 7.3103, Val: 7.2834, Test: 7.3815\n",
      "Epoch: 231, Loss: 1.3378, Train: 7.3083, Val: 7.2914, Test: 7.3898\n",
      "Epoch: 232, Loss: 1.2048, Train: 7.3148, Val: 7.3083, Test: 7.4090\n",
      "Epoch: 233, Loss: 1.3236, Train: 7.3083, Val: 7.2866, Test: 7.3841\n",
      "Epoch: 234, Loss: 1.2008, Train: 7.3101, Val: 7.2831, Test: 7.3812\n",
      "Epoch: 235, Loss: 1.2872, Train: 7.3078, Val: 7.2895, Test: 7.3874\n",
      "Epoch: 236, Loss: 1.1867, Train: 7.3123, Val: 7.3032, Test: 7.4030\n",
      "Epoch: 237, Loss: 1.2757, Train: 7.3086, Val: 7.2847, Test: 7.3821\n",
      "Epoch: 238, Loss: 1.1974, Train: 7.3095, Val: 7.2829, Test: 7.3810\n",
      "Epoch: 239, Loss: 1.2281, Train: 7.3085, Val: 7.2936, Test: 7.3921\n",
      "Epoch: 240, Loss: 1.2007, Train: 7.3086, Val: 7.2943, Test: 7.3929\n",
      "Epoch: 241, Loss: 1.2008, Train: 7.3091, Val: 7.2835, Test: 7.3814\n",
      "Epoch: 242, Loss: 1.1997, Train: 7.3083, Val: 7.2851, Test: 7.3825\n",
      "Epoch: 243, Loss: 1.1827, Train: 7.3091, Val: 7.2965, Test: 7.3957\n",
      "Epoch: 244, Loss: 1.1977, Train: 7.3076, Val: 7.2907, Test: 7.3887\n",
      "Epoch: 245, Loss: 1.1650, Train: 7.3090, Val: 7.2837, Test: 7.3816\n",
      "Epoch: 246, Loss: 1.1873, Train: 7.3077, Val: 7.2875, Test: 7.3849\n",
      "Epoch: 247, Loss: 1.1598, Train: 7.3087, Val: 7.2957, Test: 7.3950\n",
      "Epoch: 248, Loss: 1.1796, Train: 7.3075, Val: 7.2895, Test: 7.3871\n",
      "Epoch: 249, Loss: 1.1551, Train: 7.3081, Val: 7.2856, Test: 7.3829\n",
      "Epoch: 250, Loss: 1.1611, Train: 7.3074, Val: 7.2898, Test: 7.3874\n",
      "Epoch: 251, Loss: 1.1561, Train: 7.3076, Val: 7.2918, Test: 7.3901\n",
      "Epoch: 252, Loss: 1.1509, Train: 7.3076, Val: 7.2876, Test: 7.3848\n",
      "Epoch: 253, Loss: 1.1579, Train: 7.3074, Val: 7.2901, Test: 7.3878\n",
      "Epoch: 254, Loss: 1.1406, Train: 7.3074, Val: 7.2910, Test: 7.3890\n",
      "Epoch: 255, Loss: 1.1470, Train: 7.3077, Val: 7.2867, Test: 7.3838\n",
      "Epoch: 256, Loss: 1.1423, Train: 7.3074, Val: 7.2885, Test: 7.3858\n",
      "Epoch: 257, Loss: 1.1357, Train: 7.3077, Val: 7.2928, Test: 7.3913\n",
      "Epoch: 258, Loss: 1.1427, Train: 7.3074, Val: 7.2880, Test: 7.3851\n",
      "Epoch: 259, Loss: 1.1329, Train: 7.3075, Val: 7.2875, Test: 7.3847\n",
      "Epoch: 260, Loss: 1.1320, Train: 7.3073, Val: 7.2909, Test: 7.3888\n",
      "Epoch: 261, Loss: 1.1325, Train: 7.3073, Val: 7.2887, Test: 7.3859\n",
      "Epoch: 262, Loss: 1.1267, Train: 7.3073, Val: 7.2889, Test: 7.3860\n",
      "Epoch: 263, Loss: 1.1252, Train: 7.3073, Val: 7.2907, Test: 7.3885\n",
      "Epoch: 264, Loss: 1.1257, Train: 7.3074, Val: 7.2875, Test: 7.3846\n",
      "Epoch: 265, Loss: 1.1228, Train: 7.3073, Val: 7.2885, Test: 7.3857\n",
      "Epoch: 266, Loss: 1.1192, Train: 7.3074, Val: 7.2916, Test: 7.3897\n",
      "Epoch: 267, Loss: 1.1220, Train: 7.3073, Val: 7.2884, Test: 7.3856\n",
      "Epoch: 268, Loss: 1.1170, Train: 7.3072, Val: 7.2884, Test: 7.3855\n",
      "Epoch: 269, Loss: 1.1160, Train: 7.3072, Val: 7.2906, Test: 7.3883\n",
      "Epoch: 270, Loss: 1.1153, Train: 7.3072, Val: 7.2890, Test: 7.3862\n",
      "Epoch: 271, Loss: 1.1143, Train: 7.3072, Val: 7.2897, Test: 7.3870\n",
      "Epoch: 272, Loss: 1.1106, Train: 7.3071, Val: 7.2897, Test: 7.3870\n",
      "Epoch: 273, Loss: 1.1111, Train: 7.3072, Val: 7.2882, Test: 7.3852\n",
      "Epoch: 274, Loss: 1.1098, Train: 7.3072, Val: 7.2903, Test: 7.3878\n",
      "Epoch: 275, Loss: 1.1082, Train: 7.3071, Val: 7.2897, Test: 7.3871\n",
      "Epoch: 276, Loss: 1.1063, Train: 7.3072, Val: 7.2882, Test: 7.3853\n",
      "Epoch: 277, Loss: 1.1066, Train: 7.3071, Val: 7.2901, Test: 7.3877\n",
      "Epoch: 278, Loss: 1.1051, Train: 7.3071, Val: 7.2895, Test: 7.3869\n",
      "Epoch: 279, Loss: 1.1041, Train: 7.3071, Val: 7.2896, Test: 7.3870\n",
      "Epoch: 280, Loss: 1.1025, Train: 7.3071, Val: 7.2895, Test: 7.3868\n",
      "Epoch: 281, Loss: 1.1021, Train: 7.3071, Val: 7.2891, Test: 7.3863\n",
      "Epoch: 282, Loss: 1.1012, Train: 7.3071, Val: 7.2904, Test: 7.3881\n",
      "Epoch: 283, Loss: 1.1008, Train: 7.3071, Val: 7.2886, Test: 7.3858\n",
      "Epoch: 284, Loss: 1.1000, Train: 7.3071, Val: 7.2898, Test: 7.3873\n",
      "Epoch: 285, Loss: 1.0991, Train: 7.3071, Val: 7.2893, Test: 7.3867\n",
      "Epoch: 286, Loss: 1.0980, Train: 7.3071, Val: 7.2900, Test: 7.3875\n",
      "Epoch: 287, Loss: 1.0971, Train: 7.3071, Val: 7.2892, Test: 7.3866\n",
      "Epoch: 288, Loss: 1.0965, Train: 7.3071, Val: 7.2895, Test: 7.3869\n",
      "Epoch: 289, Loss: 1.0956, Train: 7.3071, Val: 7.2898, Test: 7.3874\n",
      "Epoch: 290, Loss: 1.0951, Train: 7.3071, Val: 7.2893, Test: 7.3866\n",
      "Epoch: 291, Loss: 1.0943, Train: 7.3070, Val: 7.2894, Test: 7.3868\n",
      "Epoch: 292, Loss: 1.0939, Train: 7.3070, Val: 7.2893, Test: 7.3866\n",
      "Epoch: 293, Loss: 1.0932, Train: 7.3070, Val: 7.2898, Test: 7.3873\n",
      "Epoch: 294, Loss: 1.0927, Train: 7.3070, Val: 7.2890, Test: 7.3863\n",
      "Epoch: 295, Loss: 1.0921, Train: 7.3070, Val: 7.2897, Test: 7.3873\n",
      "Epoch: 296, Loss: 1.0916, Train: 7.3070, Val: 7.2892, Test: 7.3867\n",
      "Epoch: 297, Loss: 1.0912, Train: 7.3070, Val: 7.2899, Test: 7.3875\n",
      "Epoch: 298, Loss: 1.0907, Train: 7.3070, Val: 7.2887, Test: 7.3860\n",
      "Epoch: 299, Loss: 1.0905, Train: 7.3070, Val: 7.2902, Test: 7.3879\n",
      "Epoch: 300, Loss: 1.0902, Train: 7.3070, Val: 7.2885, Test: 7.3857\n",
      "Epoch: 301, Loss: 1.0902, Train: 7.3070, Val: 7.2904, Test: 7.3883\n",
      "Epoch: 302, Loss: 1.0907, Train: 7.3071, Val: 7.2878, Test: 7.3850\n",
      "Epoch: 303, Loss: 1.0916, Train: 7.3071, Val: 7.2912, Test: 7.3895\n",
      "Epoch: 304, Loss: 1.0928, Train: 7.3072, Val: 7.2870, Test: 7.3842\n",
      "Epoch: 305, Loss: 1.0947, Train: 7.3072, Val: 7.2917, Test: 7.3901\n",
      "Epoch: 306, Loss: 1.0954, Train: 7.3072, Val: 7.2869, Test: 7.3841\n",
      "Epoch: 307, Loss: 1.0949, Train: 7.3071, Val: 7.2915, Test: 7.3898\n",
      "Epoch: 308, Loss: 1.0914, Train: 7.3070, Val: 7.2881, Test: 7.3854\n",
      "Epoch: 309, Loss: 1.0878, Train: 7.3070, Val: 7.2898, Test: 7.3876\n",
      "Epoch: 310, Loss: 1.0853, Train: 7.3070, Val: 7.2902, Test: 7.3881\n",
      "Epoch: 311, Loss: 1.0854, Train: 7.3070, Val: 7.2879, Test: 7.3852\n",
      "Epoch: 312, Loss: 1.0872, Train: 7.3071, Val: 7.2912, Test: 7.3895\n",
      "Epoch: 313, Loss: 1.0885, Train: 7.3071, Val: 7.2875, Test: 7.3847\n",
      "Epoch: 314, Loss: 1.0885, Train: 7.3070, Val: 7.2910, Test: 7.3892\n",
      "Epoch: 315, Loss: 1.0866, Train: 7.3070, Val: 7.2883, Test: 7.3856\n",
      "Epoch: 316, Loss: 1.0843, Train: 7.3069, Val: 7.2898, Test: 7.3875\n",
      "Epoch: 317, Loss: 1.0827, Train: 7.3069, Val: 7.2898, Test: 7.3875\n",
      "Epoch: 318, Loss: 1.0824, Train: 7.3070, Val: 7.2885, Test: 7.3858\n",
      "Epoch: 319, Loss: 1.0828, Train: 7.3070, Val: 7.2906, Test: 7.3887\n",
      "Epoch: 320, Loss: 1.0834, Train: 7.3070, Val: 7.2880, Test: 7.3853\n",
      "Epoch: 321, Loss: 1.0836, Train: 7.3070, Val: 7.2907, Test: 7.3888\n",
      "Epoch: 322, Loss: 1.0830, Train: 7.3070, Val: 7.2883, Test: 7.3856\n",
      "Epoch: 323, Loss: 1.0821, Train: 7.3069, Val: 7.2902, Test: 7.3881\n",
      "Epoch: 324, Loss: 1.0809, Train: 7.3069, Val: 7.2892, Test: 7.3867\n",
      "Epoch: 325, Loss: 1.0801, Train: 7.3069, Val: 7.2894, Test: 7.3870\n",
      "Epoch: 326, Loss: 1.0796, Train: 7.3069, Val: 7.2899, Test: 7.3876\n",
      "Epoch: 327, Loss: 1.0796, Train: 7.3069, Val: 7.2886, Test: 7.3860\n",
      "Epoch: 328, Loss: 1.0798, Train: 7.3069, Val: 7.2906, Test: 7.3886\n",
      "Epoch: 329, Loss: 1.0805, Train: 7.3070, Val: 7.2878, Test: 7.3850\n",
      "Epoch: 330, Loss: 1.0814, Train: 7.3070, Val: 7.2912, Test: 7.3896\n",
      "Epoch: 331, Loss: 1.0826, Train: 7.3071, Val: 7.2872, Test: 7.3844\n",
      "Epoch: 332, Loss: 1.0837, Train: 7.3071, Val: 7.2917, Test: 7.3903\n",
      "Epoch: 333, Loss: 1.0841, Train: 7.3071, Val: 7.2870, Test: 7.3843\n",
      "Epoch: 334, Loss: 1.0838, Train: 7.3070, Val: 7.2915, Test: 7.3900\n",
      "Epoch: 335, Loss: 1.0817, Train: 7.3070, Val: 7.2880, Test: 7.3853\n",
      "Epoch: 336, Loss: 1.0796, Train: 7.3069, Val: 7.2904, Test: 7.3884\n",
      "Epoch: 337, Loss: 1.0776, Train: 7.3069, Val: 7.2892, Test: 7.3868\n",
      "Epoch: 338, Loss: 1.0765, Train: 7.3069, Val: 7.2893, Test: 7.3869\n",
      "Epoch: 339, Loss: 1.0762, Train: 7.3069, Val: 7.2902, Test: 7.3881\n",
      "Epoch: 340, Loss: 1.0765, Train: 7.3069, Val: 7.2882, Test: 7.3856\n",
      "Epoch: 341, Loss: 1.0772, Train: 7.3069, Val: 7.2909, Test: 7.3892\n",
      "Epoch: 342, Loss: 1.0780, Train: 7.3070, Val: 7.2876, Test: 7.3849\n",
      "Epoch: 343, Loss: 1.0789, Train: 7.3070, Val: 7.2914, Test: 7.3898\n",
      "Epoch: 344, Loss: 1.0794, Train: 7.3071, Val: 7.2872, Test: 7.3845\n",
      "Epoch: 345, Loss: 1.0800, Train: 7.3070, Val: 7.2917, Test: 7.3903\n",
      "Epoch: 346, Loss: 1.0799, Train: 7.3071, Val: 7.2871, Test: 7.3844\n",
      "Epoch: 347, Loss: 1.0799, Train: 7.3070, Val: 7.2916, Test: 7.3902\n",
      "Epoch: 348, Loss: 1.0790, Train: 7.3070, Val: 7.2876, Test: 7.3849\n",
      "Epoch: 349, Loss: 1.0779, Train: 7.3070, Val: 7.2912, Test: 7.3896\n",
      "Epoch: 350, Loss: 1.0765, Train: 7.3069, Val: 7.2882, Test: 7.3855\n",
      "Epoch: 351, Loss: 1.0752, Train: 7.3069, Val: 7.2905, Test: 7.3886\n",
      "Epoch: 352, Loss: 1.0741, Train: 7.3068, Val: 7.2889, Test: 7.3864\n",
      "Epoch: 353, Loss: 1.0733, Train: 7.3068, Val: 7.2898, Test: 7.3877\n",
      "Epoch: 354, Loss: 1.0728, Train: 7.3068, Val: 7.2894, Test: 7.3870\n",
      "Epoch: 355, Loss: 1.0724, Train: 7.3068, Val: 7.2894, Test: 7.3870\n",
      "Epoch: 356, Loss: 1.0722, Train: 7.3068, Val: 7.2898, Test: 7.3876\n",
      "Epoch: 357, Loss: 1.0721, Train: 7.3068, Val: 7.2888, Test: 7.3863\n",
      "Epoch: 358, Loss: 1.0722, Train: 7.3069, Val: 7.2904, Test: 7.3885\n",
      "Epoch: 359, Loss: 1.0726, Train: 7.3069, Val: 7.2879, Test: 7.3853\n",
      "Epoch: 360, Loss: 1.0737, Train: 7.3070, Val: 7.2915, Test: 7.3902\n",
      "Epoch: 361, Loss: 1.0762, Train: 7.3072, Val: 7.2863, Test: 7.3835\n",
      "Epoch: 362, Loss: 1.0815, Train: 7.3075, Val: 7.2940, Test: 7.3933\n",
      "Epoch: 363, Loss: 1.0909, Train: 7.3081, Val: 7.2838, Test: 7.3813\n",
      "Epoch: 364, Loss: 1.1085, Train: 7.3088, Val: 7.2977, Test: 7.3978\n",
      "Epoch: 365, Loss: 1.1230, Train: 7.3087, Val: 7.2825, Test: 7.3803\n",
      "Epoch: 366, Loss: 1.1535, Train: 7.3099, Val: 7.3005, Test: 7.4010\n",
      "Epoch: 367, Loss: 1.1472, Train: 7.3086, Val: 7.2829, Test: 7.3806\n",
      "Epoch: 368, Loss: 1.1377, Train: 7.3079, Val: 7.2952, Test: 7.3949\n",
      "Epoch: 369, Loss: 1.0944, Train: 7.3069, Val: 7.2898, Test: 7.3878\n",
      "Epoch: 370, Loss: 1.0725, Train: 7.3074, Val: 7.2856, Test: 7.3829\n",
      "Epoch: 371, Loss: 1.0877, Train: 7.3084, Val: 7.2968, Test: 7.3966\n",
      "Epoch: 372, Loss: 1.1078, Train: 7.3082, Val: 7.2835, Test: 7.3812\n",
      "Epoch: 373, Loss: 1.1077, Train: 7.3074, Val: 7.2934, Test: 7.3925\n",
      "Epoch: 374, Loss: 1.0828, Train: 7.3068, Val: 7.2892, Test: 7.3867\n",
      "Epoch: 375, Loss: 1.0698, Train: 7.3071, Val: 7.2862, Test: 7.3835\n",
      "Epoch: 376, Loss: 1.0776, Train: 7.3076, Val: 7.2944, Test: 7.3936\n",
      "Epoch: 377, Loss: 1.0918, Train: 7.3080, Val: 7.2839, Test: 7.3813\n",
      "Epoch: 378, Loss: 1.0980, Train: 7.3074, Val: 7.2936, Test: 7.3927\n",
      "Epoch: 379, Loss: 1.0858, Train: 7.3069, Val: 7.2872, Test: 7.3845\n",
      "Epoch: 380, Loss: 1.0724, Train: 7.3068, Val: 7.2888, Test: 7.3862\n",
      "Epoch: 381, Loss: 1.0689, Train: 7.3071, Val: 7.2924, Test: 7.3911\n",
      "Epoch: 382, Loss: 1.0757, Train: 7.3073, Val: 7.2856, Test: 7.3828\n",
      "Epoch: 383, Loss: 1.0820, Train: 7.3072, Val: 7.2930, Test: 7.3919\n",
      "Epoch: 384, Loss: 1.0782, Train: 7.3069, Val: 7.2881, Test: 7.3854\n",
      "Epoch: 385, Loss: 1.0704, Train: 7.3068, Val: 7.2893, Test: 7.3868\n",
      "Epoch: 386, Loss: 1.0683, Train: 7.3070, Val: 7.2921, Test: 7.3907\n",
      "Epoch: 387, Loss: 1.0726, Train: 7.3071, Val: 7.2864, Test: 7.3836\n",
      "Epoch: 388, Loss: 1.0761, Train: 7.3070, Val: 7.2923, Test: 7.3910\n",
      "Epoch: 389, Loss: 1.0730, Train: 7.3068, Val: 7.2886, Test: 7.3859\n",
      "Epoch: 390, Loss: 1.0683, Train: 7.3068, Val: 7.2892, Test: 7.3866\n",
      "Epoch: 391, Loss: 1.0671, Train: 7.3069, Val: 7.2914, Test: 7.3897\n",
      "Epoch: 392, Loss: 1.0696, Train: 7.3070, Val: 7.2872, Test: 7.3845\n",
      "Epoch: 393, Loss: 1.0718, Train: 7.3069, Val: 7.2920, Test: 7.3906\n",
      "Epoch: 394, Loss: 1.0710, Train: 7.3068, Val: 7.2881, Test: 7.3854\n",
      "Epoch: 395, Loss: 1.0682, Train: 7.3067, Val: 7.2902, Test: 7.3880\n",
      "Epoch: 396, Loss: 1.0661, Train: 7.3068, Val: 7.2906, Test: 7.3885\n",
      "Epoch: 397, Loss: 1.0663, Train: 7.3068, Val: 7.2883, Test: 7.3856\n",
      "Epoch: 398, Loss: 1.0678, Train: 7.3069, Val: 7.2917, Test: 7.3902\n",
      "Epoch: 399, Loss: 1.0687, Train: 7.3068, Val: 7.2881, Test: 7.3854\n",
      "Epoch: 400, Loss: 1.0682, Train: 7.3068, Val: 7.2911, Test: 7.3893\n",
      "Epoch: 401, Loss: 1.0664, Train: 7.3067, Val: 7.2896, Test: 7.3872\n",
      "Epoch: 402, Loss: 1.0652, Train: 7.3067, Val: 7.2895, Test: 7.3870\n",
      "Epoch: 403, Loss: 1.0651, Train: 7.3068, Val: 7.2910, Test: 7.3891\n",
      "Epoch: 404, Loss: 1.0658, Train: 7.3068, Val: 7.2884, Test: 7.3856\n",
      "Epoch: 405, Loss: 1.0666, Train: 7.3068, Val: 7.2915, Test: 7.3898\n",
      "Epoch: 406, Loss: 1.0668, Train: 7.3068, Val: 7.2884, Test: 7.3856\n",
      "Epoch: 407, Loss: 1.0663, Train: 7.3068, Val: 7.2911, Test: 7.3892\n",
      "Epoch: 408, Loss: 1.0655, Train: 7.3067, Val: 7.2892, Test: 7.3866\n",
      "Epoch: 409, Loss: 1.0646, Train: 7.3067, Val: 7.2903, Test: 7.3880\n",
      "Epoch: 410, Loss: 1.0640, Train: 7.3067, Val: 7.2903, Test: 7.3880\n",
      "Epoch: 411, Loss: 1.0638, Train: 7.3067, Val: 7.2895, Test: 7.3870\n",
      "Epoch: 412, Loss: 1.0640, Train: 7.3068, Val: 7.2909, Test: 7.3890\n",
      "Epoch: 413, Loss: 1.0642, Train: 7.3068, Val: 7.2889, Test: 7.3862\n",
      "Epoch: 414, Loss: 1.0645, Train: 7.3068, Val: 7.2913, Test: 7.3895\n",
      "Epoch: 415, Loss: 1.0647, Train: 7.3068, Val: 7.2887, Test: 7.3860\n",
      "Epoch: 416, Loss: 1.0647, Train: 7.3068, Val: 7.2913, Test: 7.3895\n",
      "Epoch: 417, Loss: 1.0645, Train: 7.3068, Val: 7.2887, Test: 7.3860\n",
      "Epoch: 418, Loss: 1.0643, Train: 7.3068, Val: 7.2912, Test: 7.3893\n",
      "Epoch: 419, Loss: 1.0640, Train: 7.3068, Val: 7.2888, Test: 7.3862\n",
      "Epoch: 420, Loss: 1.0637, Train: 7.3067, Val: 7.2910, Test: 7.3891\n",
      "Epoch: 421, Loss: 1.0635, Train: 7.3067, Val: 7.2889, Test: 7.3863\n",
      "Epoch: 422, Loss: 1.0633, Train: 7.3067, Val: 7.2909, Test: 7.3890\n",
      "Epoch: 423, Loss: 1.0631, Train: 7.3067, Val: 7.2890, Test: 7.3864\n",
      "Epoch: 424, Loss: 1.0629, Train: 7.3067, Val: 7.2908, Test: 7.3889\n",
      "Epoch: 425, Loss: 1.0627, Train: 7.3067, Val: 7.2891, Test: 7.3865\n",
      "Epoch: 426, Loss: 1.0626, Train: 7.3067, Val: 7.2910, Test: 7.3891\n",
      "Epoch: 427, Loss: 1.0627, Train: 7.3068, Val: 7.2887, Test: 7.3861\n",
      "Epoch: 428, Loss: 1.0629, Train: 7.3068, Val: 7.2914, Test: 7.3896\n",
      "Epoch: 429, Loss: 1.0634, Train: 7.3068, Val: 7.2881, Test: 7.3853\n",
      "Epoch: 430, Loss: 1.0643, Train: 7.3069, Val: 7.2921, Test: 7.3907\n",
      "Epoch: 431, Loss: 1.0657, Train: 7.3070, Val: 7.2870, Test: 7.3842\n",
      "Epoch: 432, Loss: 1.0681, Train: 7.3071, Val: 7.2935, Test: 7.3924\n",
      "Epoch: 433, Loss: 1.0718, Train: 7.3074, Val: 7.2855, Test: 7.3826\n",
      "Epoch: 434, Loss: 1.0786, Train: 7.3077, Val: 7.2958, Test: 7.3952\n",
      "Epoch: 435, Loss: 1.0866, Train: 7.3082, Val: 7.2837, Test: 7.3813\n",
      "Epoch: 436, Loss: 1.1007, Train: 7.3087, Val: 7.2985, Test: 7.3985\n",
      "Epoch: 437, Loss: 1.1087, Train: 7.3084, Val: 7.2831, Test: 7.3806\n",
      "Epoch: 438, Loss: 1.1222, Train: 7.3091, Val: 7.2996, Test: 7.3997\n",
      "Epoch: 439, Loss: 1.1173, Train: 7.3083, Val: 7.2834, Test: 7.3809\n",
      "Epoch: 440, Loss: 1.1119, Train: 7.3080, Val: 7.2963, Test: 7.3959\n",
      "Epoch: 441, Loss: 1.0889, Train: 7.3070, Val: 7.2868, Test: 7.3840\n",
      "Epoch: 442, Loss: 1.0696, Train: 7.3067, Val: 7.2899, Test: 7.3876\n",
      "Epoch: 443, Loss: 1.0615, Train: 7.3070, Val: 7.2928, Test: 7.3916\n",
      "Epoch: 444, Loss: 1.0682, Train: 7.3075, Val: 7.2848, Test: 7.3820\n",
      "Epoch: 445, Loss: 1.0814, Train: 7.3078, Val: 7.2957, Test: 7.3951\n",
      "Epoch: 446, Loss: 1.0877, Train: 7.3078, Val: 7.2841, Test: 7.3815\n",
      "Epoch: 447, Loss: 1.0879, Train: 7.3074, Val: 7.2943, Test: 7.3934\n",
      "Epoch: 448, Loss: 1.0779, Train: 7.3070, Val: 7.2864, Test: 7.3836\n",
      "Epoch: 449, Loss: 1.0678, Train: 7.3067, Val: 7.2906, Test: 7.3885\n",
      "Epoch: 450, Loss: 1.0612, Train: 7.3067, Val: 7.2904, Test: 7.3882\n",
      "Epoch: 451, Loss: 1.0607, Train: 7.3069, Val: 7.2871, Test: 7.3843\n",
      "Epoch: 452, Loss: 1.0648, Train: 7.3071, Val: 7.2931, Test: 7.3919\n",
      "Epoch: 453, Loss: 1.0696, Train: 7.3072, Val: 7.2858, Test: 7.3829\n",
      "Epoch: 454, Loss: 1.0720, Train: 7.3071, Val: 7.2932, Test: 7.3922\n",
      "Epoch: 455, Loss: 1.0696, Train: 7.3069, Val: 7.2871, Test: 7.3843\n",
      "Epoch: 456, Loss: 1.0652, Train: 7.3067, Val: 7.2910, Test: 7.3893\n",
      "Epoch: 457, Loss: 1.0609, Train: 7.3067, Val: 7.2899, Test: 7.3876\n",
      "Epoch: 458, Loss: 1.0594, Train: 7.3067, Val: 7.2886, Test: 7.3860\n",
      "Epoch: 459, Loss: 1.0606, Train: 7.3068, Val: 7.2919, Test: 7.3904\n",
      "Epoch: 460, Loss: 1.0628, Train: 7.3069, Val: 7.2871, Test: 7.3844\n",
      "Epoch: 461, Loss: 1.0646, Train: 7.3069, Val: 7.2925, Test: 7.3912\n",
      "Epoch: 462, Loss: 1.0648, Train: 7.3069, Val: 7.2873, Test: 7.3846\n",
      "Epoch: 463, Loss: 1.0638, Train: 7.3068, Val: 7.2918, Test: 7.3902\n",
      "Epoch: 464, Loss: 1.0620, Train: 7.3067, Val: 7.2884, Test: 7.3858\n",
      "Epoch: 465, Loss: 1.0602, Train: 7.3067, Val: 7.2906, Test: 7.3885\n",
      "Epoch: 466, Loss: 1.0590, Train: 7.3067, Val: 7.2899, Test: 7.3875\n",
      "Epoch: 467, Loss: 1.0585, Train: 7.3067, Val: 7.2894, Test: 7.3869\n",
      "Epoch: 468, Loss: 1.0587, Train: 7.3067, Val: 7.2910, Test: 7.3892\n",
      "Epoch: 469, Loss: 1.0593, Train: 7.3067, Val: 7.2884, Test: 7.3857\n",
      "Epoch: 470, Loss: 1.0602, Train: 7.3068, Val: 7.2918, Test: 7.3902\n",
      "Epoch: 471, Loss: 1.0610, Train: 7.3068, Val: 7.2878, Test: 7.3850\n",
      "Epoch: 472, Loss: 1.0619, Train: 7.3068, Val: 7.2923, Test: 7.3909\n",
      "Epoch: 473, Loss: 1.0623, Train: 7.3069, Val: 7.2875, Test: 7.3847\n",
      "Epoch: 474, Loss: 1.0629, Train: 7.3069, Val: 7.2925, Test: 7.3911\n",
      "Epoch: 475, Loss: 1.0630, Train: 7.3069, Val: 7.2874, Test: 7.3846\n",
      "Epoch: 476, Loss: 1.0633, Train: 7.3069, Val: 7.2926, Test: 7.3913\n",
      "Epoch: 477, Loss: 1.0631, Train: 7.3069, Val: 7.2874, Test: 7.3845\n",
      "Epoch: 478, Loss: 1.0632, Train: 7.3069, Val: 7.2926, Test: 7.3912\n",
      "Epoch: 479, Loss: 1.0629, Train: 7.3069, Val: 7.2874, Test: 7.3846\n",
      "Epoch: 480, Loss: 1.0629, Train: 7.3069, Val: 7.2926, Test: 7.3912\n",
      "Epoch: 481, Loss: 1.0627, Train: 7.3069, Val: 7.2874, Test: 7.3846\n",
      "Epoch: 482, Loss: 1.0628, Train: 7.3069, Val: 7.2927, Test: 7.3913\n",
      "Epoch: 483, Loss: 1.0628, Train: 7.3069, Val: 7.2873, Test: 7.3844\n",
      "Epoch: 484, Loss: 1.0633, Train: 7.3069, Val: 7.2929, Test: 7.3916\n",
      "Epoch: 485, Loss: 1.0636, Train: 7.3069, Val: 7.2870, Test: 7.3841\n",
      "Epoch: 486, Loss: 1.0645, Train: 7.3070, Val: 7.2933, Test: 7.3921\n",
      "Epoch: 487, Loss: 1.0652, Train: 7.3070, Val: 7.2865, Test: 7.3835\n",
      "Epoch: 488, Loss: 1.0669, Train: 7.3071, Val: 7.2939, Test: 7.3929\n",
      "Epoch: 489, Loss: 1.0686, Train: 7.3072, Val: 7.2858, Test: 7.3828\n",
      "Epoch: 490, Loss: 1.0717, Train: 7.3074, Val: 7.2949, Test: 7.3941\n",
      "Epoch: 491, Loss: 1.0747, Train: 7.3076, Val: 7.2848, Test: 7.3819\n",
      "Epoch: 492, Loss: 1.0803, Train: 7.3079, Val: 7.2964, Test: 7.3959\n",
      "Epoch: 493, Loss: 1.0852, Train: 7.3080, Val: 7.2838, Test: 7.3812\n",
      "Epoch: 494, Loss: 1.0938, Train: 7.3085, Val: 7.2981, Test: 7.3978\n",
      "Epoch: 495, Loss: 1.0993, Train: 7.3082, Val: 7.2833, Test: 7.3807\n",
      "Epoch: 496, Loss: 1.1087, Train: 7.3088, Val: 7.2990, Test: 7.3989\n",
      "Epoch: 497, Loss: 1.1083, Train: 7.3082, Val: 7.2833, Test: 7.3806\n",
      "Epoch: 498, Loss: 1.1088, Train: 7.3084, Val: 7.2977, Test: 7.3974\n",
      "Epoch: 499, Loss: 1.0960, Train: 7.3076, Val: 7.2847, Test: 7.3818\n",
      "Epoch: 500, Loss: 1.0808, Train: 7.3070, Val: 7.2930, Test: 7.3917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import MovieLens\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "device = torch.device('cuda:3')\n",
    "if True:\n",
    "    weight = torch.bincount(train_data['user', 'movie'].edge_label.flatten())\n",
    "    weight = weight.max() / weight\n",
    "else:\n",
    "    weight = None\n",
    "\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()\n",
    "\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['user'][row], z_dict['movie'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "# class Model(torch.nn.Module):\n",
    "#     def __init__(self, hidden_channels):\n",
    "#         super().__init__()\n",
    "#         self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "#         self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "#         self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "#     def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "#         z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "#         return self.decoder(z_dict, edge_label_index)\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "    def get_embeddings(self, x_dict, edge_index_dict):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return z_dict\n",
    "\n",
    "\n",
    "model = Model(hidden_channels=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['user', 'movie'].edge_label_index)\n",
    "    target = train_data['user', 'movie'].edge_label\n",
    "    loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['user', 'movie'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data['user', 'movie'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)\n",
    "\n",
    "\n",
    "for epoch in range(1, 501):\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    test_rmse = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "          f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')\n",
    "node_embeddings = model.get_embeddings(data.x_dict, data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('hospital/triple.npy',node_embeddings['user'].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_embedding = node_embeddings['user'].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hospital/value.npy',node_embeddings['movie'].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=64, random_state=0).fit(triple_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "23    55\n",
       "56    43\n",
       "6     40\n",
       "29    38\n",
       "26    38\n",
       "      ..\n",
       "30     1\n",
       "15     1\n",
       "5      1\n",
       "24     1\n",
       "61     1\n",
       "Name: count, Length: 64, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(kmeans.predict(triple_embedding))[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0    22\n",
       "1    13\n",
       "2    45\n",
       "3    13\n",
       "4    22\n",
       "..   ..\n",
       "995  31\n",
       "996  27\n",
       "997  20\n",
       "998  38\n",
       "999  38\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(kmeans.predict(triple_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import RelLinkPredDataset\n",
    "from torch_geometric.nn import GAE, RGCNConv\n",
    "\n",
    "device = torch.device('cuda:3')\n",
    "\n",
    "class RGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, hidden_channels, num_relations):\n",
    "        super().__init__()\n",
    "        self.node_emb = Parameter(torch.empty(num_nodes, hidden_channels))\n",
    "        self.conv1 = RGCNConv(hidden_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=5)\n",
    "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=5)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.node_emb)\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        x = self.node_emb\n",
    "        x = self.conv1(x, edge_index, edge_type).relu_()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DistMultDecoder(torch.nn.Module):\n",
    "    def __init__(self, num_relations, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.rel_emb = Parameter(torch.empty(num_relations, hidden_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.rel_emb)\n",
    "\n",
    "    def forward(self, z, edge_index, edge_type):\n",
    "        z_src, z_dst = z[edge_index[0]], z[edge_index[1]]\n",
    "        rel = self.rel_emb[edge_type]\n",
    "        return torch.sum(z_src * rel * z_dst, dim=1)\n",
    "\n",
    "\n",
    "model = GAE(\n",
    "    RGCNEncoder(data.num_nodes, 500, dataset.num_relations),\n",
    "    DistMultDecoder(dataset.num_relations // 2, 500),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def negative_sampling(edge_index, num_nodes):\n",
    "    # Sample edges by corrupting either the subject or the object of each edge.\n",
    "    mask_1 = torch.rand(edge_index.size(1)) < 0.5\n",
    "    mask_2 = ~mask_1\n",
    "\n",
    "    neg_edge_index = edge_index.clone()\n",
    "    neg_edge_index[0, mask_1] = torch.randint(num_nodes, (mask_1.sum(), ),\n",
    "                                              device=neg_edge_index.device)\n",
    "    neg_edge_index[1, mask_2] = torch.randint(num_nodes, (mask_2.sum(), ),\n",
    "                                              device=neg_edge_index.device)\n",
    "    return neg_edge_index\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model.encode(data.edge_index, data.edge_type)\n",
    "\n",
    "    pos_out = model.decode(z, data.train_edge_index, data.train_edge_type)\n",
    "\n",
    "    neg_edge_index = negative_sampling(data.train_edge_index, data.num_nodes)\n",
    "    neg_out = model.decode(z, neg_edge_index, data.train_edge_type)\n",
    "\n",
    "    out = torch.cat([pos_out, neg_out])\n",
    "    gt = torch.cat([torch.ones_like(pos_out), torch.zeros_like(neg_out)])\n",
    "    cross_entropy_loss = F.binary_cross_entropy_with_logits(out, gt)\n",
    "    reg_loss = z.pow(2).mean() + model.decoder.rel_emb.pow(2).mean()\n",
    "    loss = cross_entropy_loss + 1e-2 * reg_loss\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model.encode(data.edge_index, data.edge_type)\n",
    "\n",
    "    valid_mrr = compute_mrr(z, data.valid_edge_index, data.valid_edge_type)\n",
    "    test_mrr = compute_mrr(z, data.test_edge_index, data.test_edge_type)\n",
    "\n",
    "    return valid_mrr, test_mrr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_rank(ranks):\n",
    "    # fair ranking prediction as the average\n",
    "    # of optimistic and pessimistic ranking\n",
    "    true = ranks[0]\n",
    "    optimistic = (ranks > true).sum() + 1\n",
    "    pessimistic = (ranks >= true).sum()\n",
    "    return (optimistic + pessimistic).float() * 0.5\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_mrr(z, edge_index, edge_type):\n",
    "    ranks = []\n",
    "    for i in tqdm(range(edge_type.numel())):\n",
    "        (src, dst), rel = edge_index[:, i], edge_type[i]\n",
    "\n",
    "        # Try all nodes as tails, but delete true triplets:\n",
    "        tail_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        for (heads, tails), types in [\n",
    "            (data.train_edge_index, data.train_edge_type),\n",
    "            (data.valid_edge_index, data.valid_edge_type),\n",
    "            (data.test_edge_index, data.test_edge_type),\n",
    "        ]:\n",
    "            tail_mask[tails[(heads == src) & (types == rel)]] = False\n",
    "\n",
    "        tail = torch.arange(data.num_nodes)[tail_mask]\n",
    "        tail = torch.cat([torch.tensor([dst]), tail])\n",
    "        head = torch.full_like(tail, fill_value=src)\n",
    "        eval_edge_index = torch.stack([head, tail], dim=0)\n",
    "        eval_edge_type = torch.full_like(tail, fill_value=rel)\n",
    "\n",
    "        out = model.decode(z, eval_edge_index, eval_edge_type)\n",
    "        rank = compute_rank(out)\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # Try all nodes as heads, but delete true triplets:\n",
    "        head_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        for (heads, tails), types in [\n",
    "            (data.train_edge_index, data.train_edge_type),\n",
    "            (data.valid_edge_index, data.valid_edge_type),\n",
    "            (data.test_edge_index, data.test_edge_type),\n",
    "        ]:\n",
    "            head_mask[heads[(tails == dst) & (types == rel)]] = False\n",
    "\n",
    "        head = torch.arange(data.num_nodes)[head_mask]\n",
    "        head = torch.cat([torch.tensor([src]), head])\n",
    "        tail = torch.full_like(head, fill_value=dst)\n",
    "        eval_edge_index = torch.stack([head, tail], dim=0)\n",
    "        eval_edge_type = torch.full_like(head, fill_value=rel)\n",
    "\n",
    "        out = model.decode(z, eval_edge_index, eval_edge_type)\n",
    "        rank = compute_rank(out)\n",
    "        ranks.append(rank)\n",
    "\n",
    "    return (1. / torch.tensor(ranks, dtype=torch.float)).mean()\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 10001):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:05d}, Loss: {loss:.4f}')\n",
    "    if (epoch % 500) == 0:\n",
    "        valid_mrr, test_mrr = test()\n",
    "        print(f'Val MRR: {valid_mrr:.4f}, Test MRR: {test_mrr:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1502, -0.1701,  0.1920,  ...,  0.0824, -0.3922, -0.0567],\n",
       "        [-0.1331, -0.1591,  0.2004,  ...,  0.0896, -0.3768, -0.0567],\n",
       "        [-0.1532, -0.1559,  0.1969,  ...,  0.0989, -0.3868, -0.0499],\n",
       "        ...,\n",
       "        [-0.0696, -0.1088,  0.2003,  ...,  0.0954, -0.3104, -0.0635],\n",
       "        [-0.0364, -0.0872,  0.2047,  ...,  0.0983, -0.2964, -0.0684],\n",
       "        [-0.0320, -0.0982,  0.2122,  ...,  0.1012, -0.2960, -0.0649]],\n",
       "       device='cuda:3', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embeddings['user']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "punica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
